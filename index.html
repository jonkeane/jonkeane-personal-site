<!DOCTYPE html>

<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

<!-- Basic Page Needs
================================================== -->
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>Jonathan Keane - Data Scientist</title>
<meta name="description" content="Jonathan Keane's Professional Website" />
<meta name="author" content="Jonathan Keane" />
<meta name="keywords" content="Data science, predictive models, statistics, test driven development, software carpentry, data carpentry, data analysis, research, teaching, linguistics, ASL, American Sign Language, fingerspelling, phonetics, computational, syntax, morphology" />
<meta name="description" content="Data scientist and software developer with a PhD in Linguistics. Interests include data science, software developement, predictive models, articulatory phonetics, and computational approaches to language modeling, sign language phonetics and phonology." />
<meta name="robots" content="index, follow, noarchive" />
<meta name="googlebot" content="noarchive" />

<!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<!-- Mobile Specific Metas
================================================== -->
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<!-- CSS
================================================== -->
<link rel="stylesheet" href="stylesheets/base.css" />
<link rel="stylesheet" href="stylesheets/skeleton.css" />
<link rel="stylesheet" href="stylesheets/layout.css" />
<link rel="stylesheet" href="stylesheets/jk.css" />

<!-- Favicons
================================================== -->
<link rel="shortcut icon" href="images/favicon.ico" />
<link rel="apple-touch-icon" href="images/apple-touch-icon.png" />
<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png" />
<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png" />

<!-- Google Analytics
================================================== -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-27797895-1']);
  _gaq.push(['_setDomainName', '.jonkeane.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body>

<!-- Primary Page Layout
================================================== -->

<div class="container">
<div class="three columns sidebar">
  <nav>
  <img id="jon" alt="Picture of Jonathan Keane" src="images/jonSquareRev.png" />
  <ul id="mainnav">
    <li><a href="#">Introduction</a></li>
    <li><a href="#projects">Projects</a></li>
    <li><a href="#research">Research</a></li>
    <li><a href="#software">Software</a></li>
    <li><a href="#hardware">Hardware</a></li>
  </ul>

  <ul id="external">
    <li><a href="keane-resume.pdf">resume</a></li>
    <li><a href="http://pubs.jonkeane.com">publications</a></li>
    <li><a href="http://cv.jonkeane.com">academic cv</a></li>
    <li><a href="http://blog.jonkeane.com">blog</a></li>
    <li><a href="https://github.com/jonkeane">github</a></li>
	<li><a href="mailto:jkeane@gmail.com">email</a></li>
  </ul>
  </nav>
  <br/>
</div>
<div class="twelve columns offset-by-one">
  <header>
  <!-- <img id="mark" src="images/jk.png" /> -->
  <h1 class="remove-bottom">Jonathan Keane, PhD</h1>
 </header>
  <hr class="large" />
<div class="doc-section clearfix" id="intro">
  <!-- <h2>Introduction</h2> -->

<p>I am a data scientist driven by a strong desire to find, analyze, and predict patterns in data with a commitment and focus on reproducibility and scalability. My training has strengths in software development, predictive models, social science, as well as quantitative and statistical reasoning, giving me a unique ability to synthesize and manage quantitative data for reliable insights.</p>

</div>
<hr />

<div  class="doc-section clearfix" id="projects">
  <h2>Projects</h2>

  <p>A selected list of past and current projects.</p>

  <h3>Police Early Intervention System</h3>

	<p> <a href="https://dssg.uchicago.edu/project/expanding-our-early-intervention-system-for-adverse-police-interactions/">project webpage</a> | <a href="https://github.com/dssg/police-eis/">project github repo</a> </p>

  <p>Adverse interactions between police and the communities that they are tasked with protecting are an ongoing problem in the United States. The worst of these interactions have fatal consequences, but even interactions that don't result in death do irreparable harm and are devastating to the citizens involved. Preventing these interactions is critical to a just society and is one step in building trust between communities and police.</p>

  <p>During the summer of 2016, I was a data science fellow with <a href="https://dssg.uchicago.edu/">Data Science for Social Good</a>. I worked on a team that developed an early intervention system in partnership with the Metropolitan Nashville Police Department. This system is designed to identify police officers who are at high risk of having an adverse incident; so that the department can step in and provide additional training, resources, counseling, or take other appropriate actions in order to try and prevent adverse incidents from happening in the first place. Although it would be impossible to predict and prevent all negative interactions between police and the public, an early intervention system is one piece of a broader movement for criminal justice reform that can help lead to fewer negative interactions and better outcomes for the communities that the police serve.</p>

  <p>In addition to developing a predictive model for our partner department, we also worked closely with <a href="https://dssg.uchicago.edu/project/building-a-deeper-police-early-intervention-system/">another team</a> who was working with the Charlotte Mecklenburg Police department. Together we developed a pipeline, database schema, and model that is department-independent. This added layer of collaboration is a key proving ground that the systems we developed can be implemented not just at one department, but, given the right data, almost any department in the country.</p>

  <p>We used 5 years of internal department data (including arrest records, dispatches, patrol areas, internal and external discipline, citizen complaints, &c.) as inputs to our model. For each officer, we made a prediction (which can also be thought of as a risk score) of how at risk of being involved in an adverse incident that officer is over the next year. Our best preforming model (a variation on random forests) correctly identified 80% of officers who went on to have an adverse incident (during our test period), while only requiring intervention on 30% of officers in the department. This is a drastic improvement over using the current state of the art in most police departments (a threshold-based flagging system): which would have needed to intervene on 67% of the department for the same level of accuracy.</p>

  <p>Since the summer, the <a href="http://dsapp.uchicago.edu/">Center for Data Science and Public Policy</a> has continued work with both police departments and is in the process of implementing these models. This is the first data-driven early intervention system of its kind to be implemented in any department in the United States.</p>

</div>
<hr />



<div class="doc-section clearfix" id="research">
  <h2>Academic Research</h2>

    <p>I was a postdoctoral scholar at the <a href="http://gslcenter.uchicago.edu/">Center for Sign, Gesture, and Language</a> at the <a href="http://uchicago.edu">University of Chicago</a> from 2014 to 2016. Previously, I worked as a research assistant in the <a href="http://signlanguagelab.uchicago.edu">Sign Language Linguistics Lab</a> as well as the <a href="http://clml.uchicago.edu">Chicago Language Modeling Lab</a>. I have many research interests, particularly articulatory phonetics and phonology, morphology, and computational approaches to each. I have worked on a number of projects involving sign language phonetics and phonology, how perception and action influence gesture, and how gesture and sign languages interact.</p>

  <p>Broadly speaking, I'm interested in how humans use their bodies to communicate both linguistically and non-linguistically. My primary focus is on how signers (people who use sign languages) use their body, arms, and hands in linguistic systems. How are the infinite number of possible configurations for a given articulator divided into meaningful groups (<em>ie</em> phonological units)? How much variation is allowed within these groups? What are the factors that contribute to this variation?</p>

   <p>Since the fall of 2009, I've been working with a research group consisting of researchers who specialize in linguistics, speech and language processing, and computer vision, with the goal of developing automated sign language recognition tools. This collaboration fostered my interest in the phonetics of sign languages. I hope to continue to develop models and tools that contribute both to our knowledge of phonetics generally, and inform automatic recognizers of fingerspelling.</p>

  <p> For a current list of my publications please see <a href="http://pubs.jonkeane.com">my publications</a>.</p>

        <h3>American Sign Language (<span style="font-variant: small-caps;">asl</span>) fingerspelling</h3>

  <p>Fingerspelling is used anywhere from 12 to 35
	percent of the time in <span style="font-variant:
	small-caps;">asl</span>, (<a class="cite"
	href="http://scholar.google.com/scholar?q=%22How+the+alphabet+came+to+be+used+in+a+sign+language%22"
	title="C. Padden and D. Gunsauls. How the alphabet came to be used in a sign language. Sign Language Studies, 4:10–33, 2003.">Padden and Gunsauls, 2003</a>) and as such should not
	be set aside as extralinguistic. There has only been a small amount of information put together on the phonetics of fingerspelling. The only work on fingerspelling phonetics explicitly that I've found is (<a class="cite" href="http://scholar.google.com/scholar?q=%22The+phonetics+of+fingerspelling%22" title="S. Wilcox. The phonetics of fingerspelling. John Benjamins Publishing Company, 1992.">Wilcox 1992</a>) as well as (<a class="cite" href="http://scholar.google.com/scholar?q=%22Interarticulator+co-ordination+in+deaf+signers+with+parkinson’s+disease%22" title="M. Tyrone, J. Kegl, and H. Poizner. Interarticulator co-ordination in deaf signers with parkinson’s disease. Neuropsychologia, 37(11):1271–1283, 1999.">Tyrone et al. 1999</a>).</p>

  <p>I'm especially interested in how contextual variation can be modeled based on linguistic (<em>eg</em> articulator activation, phonological features) as well as non-linguistic (<em>eg</em> physiological) factors. To test theories of this variation (as well as others about phonetics, phonology, and their interface), I study how signers produce <span style="font-variant: small-caps;">asl</span> fingerspelling. Studying fingerspelling provides opportunities to find contextual and time-conditioned variation in handshape that are relatively limited in signing. This work builds on phonological systems of sign language production, but with a detailed focus on the specific aspects that make up handshapes in <span style="font-variant: small-caps;">asl</span>.</p>

  <p>My work continues to explore fingerspelling production. I am continuing to model handshape and temporal variation that was the focus of my dissertation. I'm also involved in projects that look at how native signers as well as second language learners perceive and comprehend fingerspelling, and especially what factors contribute to successful fingerspelling comprehension; in projects that look at how handshape similarity can be quantified and tested.</p>

  <p>I use a variety of methods including annotated video data and instrumented capture to generate large, robust, quantitative sets of data. Similar methods have a (relatively) long tradition in spoken language linguistics, however they are only beginning to be used to look at signed languages. My work is supported in part by <span style="font-variant: small-caps;">nsf bcs</span> 1251807.</p>

  <h4>Dissertation project</h4>

  <p><a href="http://pubs.jonkeane.com/papers/Keane2014ad.html">My dissertation (defended August, 2014)</a> develops an articulatory phonology model (for more information, see the more detailed description <a href="#amohs">below</a>) linking the phonology and phonetics of handshapes in American Sign Language (<span style="font-variant: small-caps;">asl</span>), which was validated against data on handshape variation. On top of handshape variation, my dissertation includes detailed analyses of temporal information of the fingerspelling of native <span style="font-variant: small-caps;">asl</span> signers.</p>

</div>
<hr />

<div  class="doc-section clearfix" id="software">
  <h2>Software</h2>

  <p>Some interesting, and hopefully helpful tools for others.</p>

  <h3>MocapGrip</h3>
	 <p><a href="https://github.com/jonkeane/mocapGrip">MocapGrip</a> is a complete data processing and analysis pipeline R package. MocapGrip contains a full pipeline for motion capture data collection, from initial data processing to statistical modelling and report generation.  The package includes functions to check human annotated data for consistency and errors, as well as a flexible analysis and reporting system for use by novice users.</p>
	 <p>This was my first large-scale foray into building a stand-alone R package. For a number of reasons, I adopted a test-drive development approach to this package. Being test driven allowed me to better address (and ensure that I was reliable in addressing) data validation as well as providing feedback to users when there were errors was consistent and accurate. As the project grew, the number of possible edge-case errors exploded. Testing each one manually was unsustainable and error-prone. Whereas, using unit (and integration) tests through the <a href="https://github.com/hadley/testthat">testthat package</a> being constantly run using <a href="https://travis-ci.com">travis <span style="font-variant: small-caps;">ci</span></a>.</p>

   <h3>Pyelan</h3>

  <p>I've developed <a href="https://github.com/jonkeane/pyelan">pyelan</a>, a python module that allows for eas[y | ier] extraction and manipulation of annotation data from <a href="http://www.lat-mpi.eu/tools/elan/">elan</a> files. Although this is a work in progress, some core functionality has been implemented. Pyelan can read, write, and preform some manipulations of eaf files. Pyelan now also allows for linking csv files to be viewed in the timeseries viewer. Please feel free to use, fork, submit issues, and submit pull requests.</p>

   <h3>PhaseSpaceHelper for PhaseSpace motion capture systems</h3>

  <p><a href="https://github.com/jonkeane/PhaseSpaceHelper">PhaseSpaceHelper</a> is a python module that contains some convenience functions to deal with synchronizing stimulus presentation and data collection (through <span style="font-variant: small-caps;">smpte</span> timecode), as well as verifying the accuracy of calibration given a set object. Warning: this is very much in active development right now.</p>

  <h3 id="amohs">The Articulatory Model of Handshape</h3>
  <p>For my dissertation, I implemented a computational model of the phonetics-phonology interface, that I call the Articulatory Model of Handshape. The implementation is as the <a href="https://github.com/jonkeane/amohs">amohs</a> python module. This module not only implements automatic translation from phonological features to various types of phonetic representations (including joint angle targets), but it also uses an external library to render <span style="font-variant: small-caps;">3d</span> images of hands.</p>

  <p>The Articulatory Model of Handshape uses a slightly modified version of <a class="cite" href="https://scholar.google.com/scholar?q=%22A+Prosodic+Model+of+Sign+Language+Phonology%22" title="D. Brentari. A Prosodic Model of Sign Language Phonology. The MIT Press, 1998.">Brentari's 1998</a> Prosodic Model of handshape for a phonological representation of handshape. It than provides representations for phonetic specifications either as tract variables (at a categorical level), and as phonetic joint angle targets (a continuous level) for handshapes. Using these representations, comparisons between handshapes can be made deriving a theory-driven metric of handshape similarity.</p>

  <p>On top of the computational implementation described above, the module uses an external library, <a href="http://www.libhand.org/">LibHand</a> to render images of synthesized handshapes. Currently, the model only renders isolated handshapes, but in the future could be extended to sequences of handshapes (including transitions), that is, video of handshapes moving over time. Because they are based on representations that can be linked to multiple levels of phonetics and phonology, these videos could include information about coarticulation (contextual dependencies) of the kind that was demonstrated in my dissertation.</p>

  <p>At the time that I was trying, <a href="http://www.libhand.org/">LibHand</a> failed to compile on modern versions of OS X. I help maintain <a href="https://github.com/libhand/libhand">a repository</a> that includes the changes needed to compile LibHand on modern linux, OS X, and windows systems. Compiling via <a href="http://brew.sh/">homebrew</a> is possible, with some <a href="https://github.com/jonkeane/homebrew-libhand">alterations</a> to ogre as well.</p>

  <h3>SLGloss LaTeX package</h3>

  <p>In collaboration with <a href="https://files.nyu.edu/ik747/public/">Itamar Kastner</a>, I've helped developed <a href="https://github.com/itamarkast/slgloss">the SLGloss LaTeX package</a> to make it easier to typeset sign language glosses. It has three main features:   </p>
<ol>
  <li> It typesets sign glosses in smallcaps to integrate typographically with surrounding text, as well as allows for non-manual markings over specific constituents.</li>
  <li> It typesets fingerspelled words in small caps with hyphens between the letters</li>
  <li>It typesets lists of individual fingerspelled letters with hyphens on either side.</li>
  </ol>

   <h3>Fflipper</h3>

  <p>I've developed <a href="https://github.com/jonkeane/fflipper">fflipper</a>, a python module that allows for extraction of clipped videos based on the annotations extracted from an <a href="http://www.lat-mpi.eu/tools/elan/">elan</a> file.</p>

<h3><span style="font-variant: small-caps;">asl</span> fingerspelling chart</h3>

<img src="images/Asl_alphabet_gallaudet.jpg" alt="Chart of ASL fingerspelling handshapes" />

<p>After seeing many charts that were licensed and reproduced with permission, I decided to recreate a fingerspelling chart and release it using a very liberal content license so researchers and educators that need this chart can use it (nearly) freely. The handshapes are based on the font from David Rakowski.</p>

<p>There a few problems with this chart. The biggest problem is that the orientation of many letters is altered to show the configuration of the fingers. In reality, all of the handshapes are made with the palming facing out, away from the signer with the exception of <span style="font-variant: small-caps;">-g-</span> (in, towards the signer), <span style="font-variant: small-caps;">-h-</span> (in, towards the signer), <span style="font-variant: small-caps;">-p-</span> (down), <span style="font-variant: small-caps;">-q-</span> (down) and the end of <span style="font-variant: small-caps;">-j-</span> (to the side)</p>

<p>Download the <a href="Asl_alphabet_gallaudet.pdf">full sized, completely vector-based PDF version</a>.</p>

<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><p>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</p>


<h3>Better type for LaTeX</h3>

<p>In the pursuit of better typography for LaTeX I've found a couple of good walkthroughs, and a couple of invaluable tools. All of the following have been tested on TeX Live 2009, 2010, and 2011 on both OS X 10.5, 10.6, and 10.7. </p>

<h4>Minion Pro</h4>
<p>I've created <a href="http://github.com/jonkeane/MinionProforLaTeX">a bash script</a> that installs Minion Pro to a local TeX Live tree with very little user intervention.
</p>

<h4>otfinst.py</h4>
<p>
John Owens has developed <a href="http://www.ece.ucdavis.edu/~jowens/code/otfinst/">a great python tool</a> that installs many OpenType fonts. The only stumbling block I found besides some font incompatibilities was assigning (making up) <a href="http://www.tex.ac.uk/tex-archive/info/fontname/fontname.pdf">Berry names</a> to the fonts that I wanted to install, which have to be added to the script. I have made up names for the following that seem to adhere most of the conventions. If anyone knows of more widespread names for these typefaces, please <a href="mailto:jonkeane@uchicago.edu">let me know</a>.
</p>

<ul>
  <li>'Neutraface Text' : 'fne'</li>
  <li>'Neutraface Display' : 'fn3'</li>
  <li>'Gotham' : 'fg7'</li>
  <li>'Gotham Rounded' : 'fg8'</li>
</ul>
</div>

<hr />
<div  class="doc-section clearfix" id="hardware">
  <h2>Hardware</h2>

  <p>I dabble in hardware development (really, mostly hacking existing products to do things I find useful).</p>

  <h3>Button board</h3>

  <p>Through the process of collecting various kinds of psycholinguistic data, I found the need to have a versatile, inexpensive feedback system for participants to use in order to interact with a computer during the course of an experiment. Although button boxes exist already, they are typically very expensive, and not of the form factor we desired for use in experiments.</p>

  <p>To solve this, I developed <a href="https://github.com/jonkeane/buttonBoard">a button board </a> based on a <a href="http://www.pjrc.com/teensy/">Teensy 2.0</a> microcontroler and a in conjunction with momentary switch (e.g. <a href="http://www.infogrip.com/products/switches/specs-switch.html">Infogrip's Specs Switch</a>). The microcontroller is flexible enough to provide the computer with virtually any type of <span style="font-variant: small-caps;">usb</span> input possible when one of up to 4 buttons are pressed.</p>

  <h3>Kindle weather and <span style="font-variant: small-caps;">cta </span> arrival times display</h3>

  <p>After seeing <a href="http://mpetroff.net/2012/09/kindle-weather-display/">a number </a><a href="https://github.com/pjimenezmateo/kindle-wallpaper">of people</a> <a href="http://hackaday.com/2013/04/01/kindle-weather-and-recycling-display/">make</a> <a href="http://www.shatteredhaven.com/2012/11/1347365-kindle-weather-display.html">various</a> persistent display devices with kindles, I decided that what I really needed was a display for weather as well as various <span style="font-variant: small-caps;">cta </span> arrival times near my apartment.</p>
  <p>I developed a <a href="https://github.com/jonkeane/kindle-weather-display">setup</a> that grabs weather from <a href="http://www.wunderground.com/weather/api/">wunderground</a> or <a href="https://developer.forecast.io/">forecast.io</a>, as well as arrival times for a limited number (5 currently, due to space restrictions of the kindle) of <span style="font-variant: small-caps;">cta </span> stops and stations. Then displays the arrivals persistently, and cycles through: current weather conditions, a 12 hour forecast, and a 5 day forecast.</p>
  <p> Not content to just tack a kindle on the wall, I built <a href="https://www.flickr.com/photos/jonkeane/sets/72157639487321246/">a wood frame</a> using a laser cutter to house the kindle and reroute the usb cable for charging.</p>

   <img class="scale-with-grid" alt="Image of the kindle weather and arrival times display, in a wood frame." src="images/kindleWeather.jpg" />


</div>
<div  class="footer clearfix" id="colophon">
  <p>(As valid as possible, given both are still evolving specifications) <a href="http://validator.w3.org/check?uri=http%3A%2F%2Fjonkeane.com">HTML5</a> and <a href="http://jigsaw.w3.org/css-validator/validator?uri=http%3A%2F%2Fjonkeane.com&amp;profile=css3&amp;usermedium=all&amp;warning=1&amp;vextwarning=true&amp;lang=en">CSS3</a>. Based on the <a href="http://www.getskeleton.com/">skeleton</a> development kit. Background image from <a href="http://subtlepatterns.com/">subtlepatterns</a>. Typeset in <a href="http://www.fontshop.com/fontlist/families/azuro/">Azuro</a>. Hosted on <a href="http://www.github.com">GitHub</a>.</p>

</div>

</div>

</div><!-- container -->

<!-- JS
================================================== -->
<script type="text/javascript" src="http://code.jquery.com/jquery-1.6.4.min.js" ></script>
<script type="text/javascript" src="javascripts/tabs.js"></script>

<!-- End Document
================================================== -->
</body>
</html>
